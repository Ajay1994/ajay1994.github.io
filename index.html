<!DOCTYPE html>
<html>
    <head>
        <title>Peihao's Homepage</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- UIkit CSS -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/uikit@3.3.1/dist/css/uikit.min.css" />

        <!-- UIkit JS -->
        <script src="https://cdn.jsdelivr.net/npm/uikit@3.3.1/dist/js/uikit.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/uikit@3.3.1/dist/js/uikit-icons.min.js"></script>

        <style>
            .uk-label{
                color: black;
                text-transform: none;
                background-color: transparent;
                border: 1px solid black;
                padding: 0px 3px;
            }
        </style>
    </head>
    <body>
        <nav class="uk-navbar-container uk-margin uk-light" style="background-color: #BF5701" uk-navbar>
            <div class="uk-navbar-left">
                <a class="uk-navbar-item uk-logo uk-text-primary" href="index.html">Ajay Jaiswal's Homepage</a>
            </div>
            <div class="uk-navbar-right">
                <ul class="uk-navbar-nav">
                    <li class="uk-active uk-visible@m"> <a href="index.html"> Home </a> </li>
                    <li class="uk-active uk-visible@m"> <a href="index.html#research"> Research </a> </li>
                    <li class="uk-active uk-visible@m"> <a href="resources.html"> Resources </a> </li>
                    <li class="uk-active uk-visible@m"> <a href="index.html#contact"> Contact </a> </li>
                    <li class="uk-active uk-visible@m"> <a href="index.html#bio"> About </a> </li>
                    <a class="uk-active uk-navbar-toggle uk-hidden@m" uk-navbar-toggle-icon href="#offcanvas-nav-primary" uk-toggle></a>
                </ul>
            </div>
        </nav>

        <div id="offcanvas-nav-primary" uk-offcanvas="overlay: true">
            <div class="uk-offcanvas-bar uk-flex uk-flex-column">
                <ul class="uk-nav uk-nav-primary uk-nav-center uk-margin-auto-vertical">
                    <li class="uk-active"> <a href="index.html"> Home </a> </li>
                    <li class="uk-active"> <a href="index.html#research"> Projects </a> </li>
                    <li class="uk-active"> <a href="resources.html"> Resources </a> </li>
                    <li class="uk-active"> <a href="index.html#contact"> Contact </a> </li>
                    <li class="uk-active"> <a href="index.html#bio"> About </a> </li>
                </ul>
            </div>
        </div>

        <!-- Header -->
        <!-- <div class="uk-section uk-flex uk-flex-center uk-section-secondary uk-light"> -->
        <div class="uk-flex uk-flex-center">
            <div class="uk-container uk-width-1-1 uk-width-3-4@m uk-padding-small" uk-grid>
            <!-- <div class="uk-container uk-container-large uk-flex-middle uk-padding-small" uk-grid> -->
                <a class="uk-flex uk-flex-center uk-width-1-1 uk-width-auto@m" href="#">
                    <div class="uk-width-1-3 uk-width-small@s uk-visible@l">
                        <img class="uk-border-circle uk-responsive-height uk-responsive-width" src="images/avatar.jpg" alt="Portrait" />
                    </div>
                </a>
                <div class="uk-width-expand uk-margin-remove">
                    <p class="uk-h1 uk-text-center uk-text-left@m uk-width-expand@m uk-margin-remove">
                        Peihao Wang
                    </p>
                    <p>
                        I am a first-year PhD student at the Department of <a href="https://www.ece.utexas.edu/">Electrical and Computer Engineering</a>, <a href="https://www.utexas.edu/">The University of Texas at Austin</a>. I am doing scientific research in the areas of deep learning, computer vision, and computational photography, under the supervision of <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Prof. Atlas Wang</a> at the <a href="https://vita-group.github.io/">VITA Group</a>.
                        Prior to that, I obtained my bachelor's degree from <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a> in 2021.
                        My current research focuses on graph representation learning, implicit neural representation, and deep learning theories.
                    </p>
                    <!-- Buttons for desktop device -->
                    <div class="uk-margin-horizontal uk-visible@m">
                        <a href="#contact" class="uk-button uk-button-default uk-margin-small-right"> Mail </a>
                        <a href="https://scholar.google.com/citations?user=fqf2tBsAAAAJ" class="uk-button uk-button-default uk-margin-small-right">Scholar </a>
                        <a href="https://github.com/peihaowang" class="uk-button uk-button-default uk-margin-small-right"> GitHub </a>
                        <a href="https://www.linkedin.com/in/peihao-wang-25a411162/" class="uk-button uk-button-default uk-margin-small-right"> LinkedIn </a>
                    </div>
                    <!-- Buttons for mobile device -->
                    <div class="uk-margin-horizontal uk-flex uk-flex-center uk-hidden@m">
                        <a href="#contact" class="uk-button uk-button-default uk-margin-small-right" uk-icon="mail"></a>
                        <a href="https://scholar.google.com/citations?user=fqf2tBsAAAAJ" class="uk-button uk-button-default uk-margin-small-right" uk-icon="google"></a>
                        <a href="https://github.com/peihaowang" class="uk-button uk-button-default uk-margin-small-right" uk-icon="github"></a>
                        <!-- <a href="https://www.linkedin.com/in/peihao-wang-25a411162/" class="uk-button uk-button-default uk-margin-small-right" uk-icon="linkedin"></a> -->
                    </div>
                </div>
            </div>
        </div>

        <!-- Body -->
        <div class="uk-flex uk-flex-center uk-margin-remove-top uk-margin-medium-bottom">
        <div class="uk-container uk-width-1-1 uk-width-3-4@m uk-padding-small">

            <p id="news" class="uk-h2 uk-text-bold">News</p>
            <ul class="uk-list uk-list-bullet uk-margin-small-bottom">
                <li>05/2022 One paper accepted to ICML.</li>
                <li>04/2022 One paper accepted to TPAMI.</li>
                <li>03/2022 Two papers accepted to CVPR (including one oral presentation).</li>
                <li>12/2021 One paper accepted to ICLR, code released.</li>
                <li>09/2021 One paper accepted to NeurIPS, code released.</li>
            </ul>

            <p id="research" class="uk-h2 uk-text-bold">Publication</p>
            (A superscript * denotes equal contribution)
            <ul class="uk-list uk-margin-small-bottom">
                <li>
                    <div class="uk-text-lead">
                        Neural Implicit Dictionary Learning via Mixture-of-Expert Training
                    </div>
                    <div> <u>Peihao Wang</u>, <a href="https://zhiwenfan.github.io/">Zhiwen Fan</a>, <a href="https://tianlong-chen.github.io/about/">Tianlong Chen</a>, <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Atlas Wang</a></div>
                    <div><i> International Conference on Machine Learning (ICML), 2022 </i></div>
                    <div> We present Neural Implicit Dictionary (NID) that learns and represents implicit neural representation as a sparse mixture of expert networks. </div>
                    <a href="#" target="_blank" class="uk-button uk-button-text">[Paper]</a>
                    <a href="#" target="_blank" class="uk-button uk-button-text">[Code]</a>
                    <!-- <img src="https://img.shields.io/github/stars/VITA-Group/Deep_GCN_Benchmarking?style=social&logo=github"><img/> -->
                </li>
                <li>
                    <div class="uk-text-lead">
                        Bag of Tricks for Training Deeper Graph Neural Networks: A Comprehensive Benchmark Study
                    </div>
                    <div> <a href="https://tianlong-chen.github.io/about/">Tianlong Chen</a>*, <a href="https://scholar.google.com/citations?user=KlvcurEAAAAJ">Kaixiong Zhou</a>*, <a href="https://scholar.google.com/citations?user=o_mzmd0AAAAJ">Keyu Duan</a>, <a href="https://wenqing-zheng.github.io/">Wenqing Zheng</a>, <u>Peihao Wang</u>, <a href="https://cs.rice.edu/~xh37/index.html">Xia Hu</a>, <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Atlas Wang</a></div>
                    <div><i> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022 </i></div>
                    <div> We present the first fair and reproducible benchmark dedicated to assessing the "tricks" of training deep GNNs. </div>
                    <a href="https://arxiv.org/abs/2108.10521" target="_blank" class="uk-button uk-button-text">[Paper]</a>
                    <a href="https://github.com/VITA-Group/Deep_GCN_Benchmarking" target="_blank" class="uk-button uk-button-text">[Code]</a>
                    <img src="https://img.shields.io/github/stars/VITA-Group/Deep_GCN_Benchmarking?style=social&logo=github"><img/>
                </li>
                <li>
                    <div class="uk-text-lead">
                        Unified Implicit Neural Stylization
                    </div>
                    <div> <a href="https://zhiwenfan.github.io/">Zhiwen Fan</a>*, <a href="https://yifanjiang.net/">Yifan Jiang</a>*, <u>Peihao Wang</u>*, <a href="https://gongxinyuu.github.io/">Xinyu Gong</a>, <a href="https://ir1d.github.io/">Dejia Xu</a>, <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Atlas Wang</a></div>
                    <div><i> arXiv preprint, 2022 </i></div>
                    <div> This work explores stylizing an implicit neural representation, using a generalized approach that can apply to various 2D and 3D representations.</div>
                    <a href="https://zhiwenfan.github.io/INS/" target="_blank" class="uk-button uk-button-text">[Project Page]</a>
                    <a href="https://arxiv.org/abs/2204.01943" target="_blank" class="uk-button uk-button-text">[Paper]</a>
                    <a href="https://github.com/VITA-Group/INS" target="_blank" class="uk-button uk-button-text">[Code]</a>
                    <img src="https://img.shields.io/github/stars/VITA-Group/INS?style=social&logo=github"><img/>
                </li>
                <li>
                    <div class="uk-text-lead">
                        SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image
                    </div>
                    <div> <a href="https://ir1d.github.io/">Dejia Xu</a>*, <a href="https://yifanjiang.net/">Yifan Jiang</a>*, <u>Peihao Wang</u>, <a href="https://zhiwenfan.github.io/">Zhiwen Fan</a>, <a href="https://www.humphreyshi.com/">Humphrey Shi</a>, <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Atlas Wang</a></div>
                    <div><i> arXiv preprint, 2022 </i></div>
                    <div> We present Single View NeRF (SinNeRF) consisting of thoughtfully designed semantic and geometry regularizations to train neural radiance field using only a single view.</div>
                    <a href="https://vita-group.github.io/SinNeRF/" target="_blank" class="uk-button uk-button-text">[Project Page]</a>
                    <a href="https://arxiv.org/abs/2204.00928" target="_blank" class="uk-button uk-button-text">[Paper]</a>
                    <a href="https://github.com/VITA-Group/SinNeRF" target="_blank" class="uk-button uk-button-text">[Code]</a>
                    <img src="https://img.shields.io/github/stars/VITA-Group/SinNeRF?style=social&logo=github"><img/>
                </li>
                <li>
                    <div class="uk-text-lead">
                        Aug-NeRF: Training Stronger Neural Radiance Fields with Triple-Level Augmentations
                    </div>
                    <div> <a href="https://tianlong-chen.github.io/about/">Tianlong Chen</a>*, <u>Peihao Wang</u>*, <a href="https://zhiwenfan.github.io/">Zhiwen Fan</a>, <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Atlas Wang</a></div>
                    <div><i> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022 </i></div>
                    <div> We propose Aug-NeRF which augments NeRF with worst-case perturbations in three distinct levels with physical grounds.</div>
                    <a href="#" target="_blank" class="uk-button uk-button-text">[Paper]</a>
                    <a href="https://github.com/VITA-Group/Aug-NeRF" target="_blank" class="uk-button uk-button-text">[Code]</a>
                    <img src="https://img.shields.io/github/stars/VITA-Group/Aug-NeRF?style=social&logo=github"><img/>
                </li>
                <li>
                    <div class="uk-text-lead">
                        CADTransformer: Panoptic Symbol Spotting Transformer for CAD Drawings
                    </div>
                    <div><a href="https://zhiwenfan.github.io/">Zhiwen Fan</a>, <a href="https://tianlong-chen.github.io/about/">Tianlong Chen</a>, <u>Peihao Wang</u>, <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Atlas Wang</a></div>
                    <div><i> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022, Oral Presentation</i></div>
                    <div> We present CADTransformer, a transformer based framework, to tackle the panoptic symbol spotting task for computer-aided design drawings.</div>
                    <a href="#" target="_blank" class="uk-button uk-button-text">[Paper]</a>
                    <a href="https://github.com/VITA-Group/" target="_blank" class="uk-button uk-button-text">[Code]</a>
                </li>
                <li>
                    <div class="uk-text-lead">
                        Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice
                    </div>
                    <div> <u>Peihao Wang</u>, <a href="https://wenqing-zheng.github.io/">Wenqing Zheng</a>, <a href="https://tianlong-chen.github.io/about/">Tianlong Chen</a>, <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Atlas Wang</a></div>
                    <div><i> International Conference on Learning Representations (ICLR), 2022 </i></div>
                    <div> We prove that self-attention is no more than low-pass filter, and propose two simple yet effective methods to counteract excessive smoothening.</div>
                    <a href="https://arxiv.org/abs/2203.05962" target="_blank" class="uk-button uk-button-text">[Paper]</a>
                    <a href="https://drive.google.com/file/d/1_GXe-xGcCCxf0CnowHVIwUm-iZhQ2iwY/view?usp=sharing" target="_blank" class="uk-button uk-button-text">[Slides]</a>
                    <a href="https://github.com/VITA-Group/ViT-Anti-Oversmoothing" target="_blank" class="uk-button uk-button-text">[Code]</a>
                    <img src="https://img.shields.io/github/stars/VITA-Group/ViT-Anti-Oversmoothing?style=social&logo=github"><img/>
                </li>
                <li>
                    <div class="uk-text-lead">
                        Delayed Propagation Transformer: A Universal Computation Engine towards Practical Control in Cyber-Physical Systems
                    </div>
                    <div> <a href="https://wenqing-zheng.github.io/">Wenqing Zheng</a>, <a href='https://scholar.google.com/citations?user=cokDelEAAAAJ'>Qiangqiang Guo</a>, <a href='https://www.haofrankyang.net/'>Hao (Frank) Yang</a>, <u>Peihao Wang</u>, <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Atlas Wang</a></div>
                    <div><i> Advances in Neural Information Processing Systems (NeurIPS), 2021 </i></div>
                    <div> We presents the Delayed Propagation Transformer (DePT) that specializes in the global modeling of CPS while taking into account the immutable constraints from the physical world. </div>
                    <a href="https://arxiv.org/abs/2110.15926" target="_blank" class="uk-button uk-button-text">[Paper]</a>
                    <a href="https://github.com/VITA-Group/DePT" target="_blank" class="uk-button uk-button-text">[Code]</a>
                    <img src="https://img.shields.io/github/stars/VITA-Group/DePT?style=social&logo=github"><img/>
                </li>
                <li>
                    <div class="uk-text-lead">
                        SoGCN: Second-Order Graph Convolutional Networks
                    </div>
                    <div> <u>Peihao Wang</u>*, <a href="http://yuehaolab.com/">Yuehao Wang</a>*, <a href="http://linhuavvv.com/">Hua Lin</a>, <a href="https://www.cis.upenn.edu/~jshi/">Jianbo Shi</a> </div>
                    <div><i> arXiv preprint, 2021 </i></div>
                    <div> We prove that second-order graph convolution is the maximally localized kernel with full representation power. </div>
                    <a href="https://arxiv.org/abs/2110.07141" target="_blank" class="uk-button uk-button-text">[Paper]</a>
                    <a href="https://drive.google.com/file/d/1Yy6uoqW7F2_GTRIu-Hp6q6bgbz6y3rTI/view?usp=sharing" target="_blank" class="uk-button uk-button-text">[Slides]</a>
                    <a href="https://github.com/yuehaowang/SoGCN" target="_blank" class="uk-button uk-button-text">[Code]</a>
                    <img src="https://img.shields.io/github/stars/yuehaowang/SoGCN?style=social&logo=github"><img/>
                </li>
                <li>
                    <div class="uk-text-lead">
                        TightCap: 3D Human Shape Capture with Clothing Tightness Field
                    </div>
                    <div> <a href="https://chenxin.tech/">Xin Chen</a>, <a href="https://scholar.google.com/citations?user=9IbbQ10AAAAJ&hl">Anqi Pang</a>, <a href="https://scholar.google.com/citations?user=fRjxdPgAAAAJ&hl=en">Wei Yang</a>, <u>Peihao Wang</u>, <a href="http://xu-lan.com/">Lan Xu</a>, <a href="http://vic.shanghaitech.edu.cn/vrvc/en/people/">Jingyi Yu</a> </div>
                    <div><i> ACM Transactions on Graphics (TOG), 2021 </i></div>
                    <div> We propose a data-driven approach to capture both the human shape and dressed garments with only a single 3D human scan, by predicting clothing tightness. </div>
                    <a href="https://chenxin.tech/files/Paper/TOG2021_TightCap/project_page_TightCap/index.htm" target="_blank" class="uk-button uk-button-text">[Project Page]</a>
                    <a href="https://arxiv.org/abs/1904.02601" target="_blank" class="uk-button uk-button-text">[Paper]</a>
                    <a href="https://github.com/ChenFengYe/TightCap" target="_blank" class="uk-button uk-button-text">[Code]</a>
                    <img src="https://img.shields.io/github/stars/ChenFengYe/TightCap?style=social&logo=github"><img/>
                </li>
            </ul>

            <p class="uk-h2 uk-text-bold">Teaching & Service</p>
            <ul class="uk-list uk-list-bullet">
                <li>
                    Conference Reviewer of ICML 2022, ECCV 2022
                </li>
                <li>
                    SI363 - Multi-Scale Imaging for Structural Biology (Spring, 2021): Teaching assistant.
                    <!-- <div class='uk-text-small'>With the help of Prof. Xuming He and Prof. Jingyi Yu, we can open this course at the first time to provide background knowledge for machine learning and computer vision folks to boarden their impact into the field of structural biology and microscopy.</div> -->
                </li>
                <li>
                    CS276 - Computational Photography (Fall, 2020): Guest lecture in <i>"<a href="https://drive.google.com/file/d/16DvMGw8QHOJxsJ0bcUt05DZK0fesry_W/view?usp=sharing" target="_blank">Camera and Lens</a>"</i>.
                </li>
                <li>
                    <a href="http://utechacademy.cn/" target="_blank">UTech Academy AI Camp</a> (Aug, 2020): Teaching assistant of computer vision track.
                </li>
                <li>
                    <a href="https://robotics.shanghaitech.edu.cn/courses/ca/20s/" target="_blank">CS110 - Computer Architecture</a> (Spring, 2020): Teaching assistant.
                </li>
                <li>
                    CS100 - Introduction to Programming (Fall, 2019): Teaching assistant.
                </li>
            </ul>

            <p id="contact" class="uk-h2 uk-text-bold">Contact</p>
            <div class="uk-child-width-1-2@m" uk-grid>
                <div class="uk-width-large">
                    <div class="uk-card uk-card-default uk-card-hover uk-card-body">
                        <h3 class="uk-card-title">Address</h3>
                        <p>
                            2501 Speedway <br/>
                            EER 6.852 <br/>
                            Austin, TX  <br/>
                            United States <br/>
                            78712
                        </p>
                    </div>
                </div>
                <div class="uk-width-large">
                    <div class="uk-card uk-card-primary uk-card-hover uk-card-body"  style="background-color: #BF5701">
                        <h3 class="uk-card-title">Email</h3>
                        <p>
                            <b>Academic</b> <br/>
                            peihaowang [at] utexas.edu
                            <br/><br/>
                            <b>Other Use</b> <br/>
                            wangpeihao [at] gmail.com
                        </p>
                    </div>
                </div>
            </div>

            <p id="bio", class="uk-h2 uk-text-bold">About Me</p>
            <article class="uk-article">
                <blockquote cite="#">
                    <p class="uk-margin-small-bottom">Keep thy heart with all diligence; for out of it are the issues of life.</p>
                    <footer><cite>Proverbs 4:23, KJV</cite></footer>
                </blockquote>
                <p>
                    My exploration in computer science began by twelve years old, when my father bought me a C language programming book. And I spent my leisure time developing database software during high school. After entering university, I diverted my focus onto computer vision and machine learning. After two-year flundering in 3D human-centric digitalization, I found my true passion and invested my rest undergraduate years into spectral theory and cryogenic protein imaging. Now I'm trying to use mathematical theories to solve and interpret problems in learning, vision and imaging. It is always hard to be a starter, hereby I would express my sincere gratitude to those who guided me walk through the novice village of academia.
                </p>
<!--                 <p>
                    It would be impossible to hang out my shingle as a researcher without the help of many important people. Hereby I would express my sincere gratitude to those who guided me walk through the novice village of academia. I would particularly thank <a href="https://sites.google.com/site/manolisctsakiris">Prof. Manolis C. Tsakiris</a> for enlightening me in algebra, optimization, and geometry. I would thank <a href="https://www.cis.upenn.edu/~jshi/">Prof. Jianbo Shi</a> for advising me on my first paper and giving constructive suggestions on writing skills, which I even practice for today. I would also say thanks to <a href="http://vic.shanghaitech.edu.cn/vrvc/en/people/">Prof. Jingyi Yu</a>, who constantly supported my research in 3D vision, graphics, and protein imaging throughout my whole undergraduate study.
                </p> -->
<!--                 <p>
                    I enjoy doing research, reading insightful papers, and writing computer programs. During my leisure time, I play basketball and music. I was in a terrific <a href="https://en.wikipedia.org/wiki/Hackathon">Hackthathon</a> team, where we won lots of design-sprint awards. I did not travel a lot, but I was lucky to capture the most poetic moment in my hometown city: <a href="images/sunset_on_yangtze.jpg" target="_blank">Sunset on the Yangtze River</a>.
                </p> -->
            </article>
        </div>
        </div>


        <!-- Visitor recorder -->
        <img src='http://www.wjjsoft.com/cgi-bin/_count_wph.cgi?q=profile' width=1 style='visibility: hidden' />

        <!-- Footer -->
        <div class="uk-section uk-flex uk-flex-center uk-section-secondary uk-height-small" style="background-color: #BF5701">
            <div class="uk-container uk-container-large uk-text-primary">
                <p class="uk-text-small uk-text-center">Copyright &copy; 2022 Peihao Wang. All Rights Reserved.</p>
                <p class="uk-text-small uk-text-center">Powered by <span uk-icon="uikit">UIkit</span></p>
            </div>
        </div>

    </body>
</html>


<!-- 		<div class="blurb" style="font-family: Arial, Helvetica, sans-serif;">
			<div style="width:750px; height:400px;  float:right; margin-top:20px;">
				<p style="font-size:20px;display: inline-block;">I am a Ph.D. student at the 
					<a href = "https://www.ischool.utexas.edu/" style="color:#800000">
					School of Information, UT Austin</a>. I am fortunate to be advised by 
					<b><a href = "https://yingding.ischool.utexas.edu/" style="color:#800000">Prof. Ying Ding</a></b>, and 
					<b><a href = "https://www.ece.utexas.edu/people/faculty/atlas-wang" style="color:#800000">Prof. Atlas Wang</b>.</a> I am a member of 
					<b>Visual Informatics Group
					<a href = "https://vita-group.github.io/" style="color:#800000">(VITA)</a> @UT Austin</b>. 
					My primary research revolves around Multi-Modal Systems, Model Interpretability and Optimization. I am currently working on 
					designing and improving explainability of the
					training recipe for Sparse Neural Networks. I am also developing AI algorithms 
					across computer vision, natural language processing which assists in critical medical decision-making.</p>

				<p style="font-size:20px;display: inline-block;">Prior to joining UT Austin, I completed my Masters in Computer Science from 
					<a href = "http://www.iitkgp.ac.in/" style="color:#800000">Indian Institute of Technology, Kharagpur</a>. 
					I was advised by 
					<a href = "https://cse.iitkgp.ac.in/~animeshm/" style="color:#800000">Prof. Animesh Mukherjee</a> and 
					<a href = "https://cse.iitkgp.ac.in/~pawang/" style="color:#800000">Prof. Pawan Goyal</a> at the CNeRG Lab. I have also worked as Lead Research Engineer 
					at Advanced Technology Lab, Samsung Electronics, India under the supervision of
					<a href="https://scholar.google.com/citations?hl=en&user=E6Gasu0AAAAJ&view_op=list_works&sortby=pubdate" style="color:#800000">Dr. Vijay Narayan Tiwari</a>.</p>
				<p style="font-size:20px;display: inline-block; color:red"><i>Somewhere, something incredible is waiting to be known.</i> ~Prof. Carl Sagan</p>
			</div>
			<img src="/picturedp.PNG" alt="Italian Trulli" style="width:200px;height:200px;">
			<h3>Ajay Jaiswal</h3>
			<p>PhD Student - UT Austin</p>
			<p>School of Information</p>
			<p style="font-size:12px">Office: UTA 5.410, 1616 Guadalupe St</p>
			<ul>
				   <li><a href="mailto:ajayjaiswal@utexas.edu">Email</a></li>
				   <li><a href="https://scholar.google.com/citations?user=I783HxYAAAAJ&hl=en">Google Scholar</a></li>
			</ul>
		</div> 
		<h2 style = "font-size:20px">News Updates</h2>
		<hr/>
		<div style ="font-size:14px; font-family: Arial, Helvetica, sans-serif;">
			<strong>May, 2022</strong> &emsp;Our work - 'Training Your Sparse Neural Network Better with Any Mask' got accpeted at <a href = "https://icml.cc/Conferences/2022/CallForPapers" style="color:#800000">ICML 2022</a>.</br>
			<strong>Feb, 2022</strong> &emsp; Awarded Professoional Development Award by School of Information, UT Austin </br> 
			<strong>Oct, 2021</strong> &emsp;Our work - 'RadBERT-CL: Factually-Aware Contrastive Learning ForRadiology Report Classification' got accpeted at <a href = "https://ml4health.github.io/2021/index.html" style="color:#800000">ML4H 2021</a>.</br>
			<strong>Aug, 2021</strong> &emsp;Our work - 'SCALP - Supervised Contrastive Learning for Cardiopulmonary Disease Classification and Localization in Chest X-rays using Patient Metadata' got accpeted at <a href = "https://icdm2021.auckland.ac.nz/" style="color:#800000">ICDM 2021</a>.</br>
			<strong>JuL, 2021</strong> &emsp;Winner of the <a href = "https://aihealth.ischool.utexas.edu/AIHealthChallenge/index.html" style="color:#800000">AI Health Data Challenge</a> at UT Austin. (First Prize - $1000)</br>
			<strong>Jun, 2021</strong> &emsp;Our work - 'Using Radiomics as Prior Knowledge for Thorax Disease Classification and Localization in Chest X-rays' got accpeted at AMIA 2021.</br>
			<strong>Jun, 2021</strong> &emsp;Started Working as Graduate Research Assistant with <a href = "http://www.katrinerk.com/" style="color:#800000"> Prof. Katrin Erk </a> on Narrative Inference. </br>
			<strong>Mar, 2021</strong> &emsp;Presented our paper "Understanding Parachuting Collaboration" at iConference 2021.</br>
			<strong>Jan, 2021</strong> &emsp;Started PhD in Information Science at University of Texas at Austin</br>
			<strong>Jan, 2021</strong> &emsp;Receipent of Kilgarlin Fellowship ($2,500/semester - 2021 to 2024)</br>
			<strong>Jan, 2021</strong> &emsp;Receipent of William and Margaret Kilgarlin Endowed Scholarship ($54,750 - 2021 to 2022)</br>
			<strong>Dec, 2020</strong> &emsp;Left job at Advanced Technology Lab, Samsung Electronics, Banglore, India.</br>
			<strong>Feb, 2020</strong> &emsp;Our submitted systems ranked 2nd, and 3rd place for Task 4, and 11 at 
						<a href = "https://alt.qcri.org/semeval2020/index.php?id=tasks" style="color:#800000">SemEval 2020</a>.</br>
		</div>
		<h2 style = "font-size:16px">Select Publications (*Equal Contribution)</h2>
		<hr/>
		<div style ="font-size:14px; font-family: Arial, Helvetica, sans-serif;">
			<p><strong>Training Your Sparse Neural Network Better with Any Mask</strong><br/>
			<u>Ajay Jaiswal</u>, Haoyu Ma, Tianlong Chen, Ying Ding, Ying Ding, Atlas Wang<br/>
		        International Conference on Machine Learning (ICML), 2022. 
			</p>
			<p><strong>RadBERT-CL: Factually-Aware Contrastive Learning ForRadiology Report Classification</strong><br/>
			<u>Ajay Jaiswal</u>, Liyan Tang, Meheli Ghosh, Justin Rousseau, Yifan Peng, and Ying Ding<br/>
		        2021 Machine Learning for Health (ML4H 2021)
			</p>
			<p><strong>SCALP - Supervised Contrastive Learning for Cardiopulmonary Disease Classification and Localization in Chest X-rays using Patient Metadata</strong><br/>
			<u>Ajay Jaiswal</u>, Tianhao Li, Cyprian Zander, Yan Han, Justin Rousseau, Yifan Peng, and Ying Ding<br/>
		        2021 21st IEEE International Conference on Data Mining (ICDM 2021)
			</p>
			<p><strong>Using Radiomics as Prior Knowledge for Abnormality Classification and Localization in Chest X-rays</strong><br/>
			Yan Han, Chongyan Chen, Liyan Tang, Mingquan Lin, <u>Ajay Jaiswal</u>, Ying Ding, Yifan Peng<br/>
		        2021 American Medical Informatics Association Annual Symposium 
			</p>
			<p><strong>Understanding Parachuting Collaboration</strong><br/>
			<u>Ajay Jaiswal</u>, Meijun Liu, Ying Ding<br/>
		        2021 Diversity, Divergence, Dialogue: 16th International Conference, iConference 
			</p>
			<p><strong>Ensemble Architecture for Fine-Tuned Propaganda Detection in News Articles</strong><br/>
			Mayank Raj*, <u>Ajay Jaiswal</u>*, RR Rohit, Ankita Gupta, Sudeep Kumar Sahoo, Vertika Srivastava, Yeon Hyang Kim<br/>
			2020 Proceedings of the Fourteenth Workshop on Semantic Evaluation, 1802-1807
			</p>
			<p><strong>Be Reasonable: Exploiting Large-scale Language Models for Commonsense Reasoning</strong><br/>
			Vertika Srivastava, Sudeep Kumar Sahoo, Yeon Hyang Kim, Rohit Rr, Mayank Raj, <u>Ajay Jaiswal</u><br/>
			2020 Proceedings of the Fourteenth Workshop on Semantic Evaluation, 585-593
			</p>
			<p><strong>Understanding the impact of early citers on long-term scientific impact</strong><br/>
			Mayank Singh, <u>Ajay Jaiswal</u>, Priya Shree, Arindam Pal, Animesh Mukherjee, Pawan Goyal<br/>
			2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL)
			</p>
		</div>
 -->
